services:
  llama-api-server:
    image: ubuntu:22.04
    container_name: llama-api-server
    ports:
      - "9068:9068"
    environment:
      - DEBIAN_FRONTEND=noninteractive
    volumes:
      - ./models:/models:ro
      - ./logs:/logs
    command: >
      bash -c "
      apt-get update && apt-get install -y curl wget &&
      curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.1 --ggmlbn=b5640 &&
      export PATH=\$PATH:\$HOME/.wasmedge/bin &&
      cd /tmp &&
      curl -LO https://github.com/LlamaEdge/LlamaEdge/releases/download/0.22.1/llama-api-server.wasm &&
      curl -LO https://huggingface.co/second-state/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_0.gguf &&
      mkdir -p /models &&
      mv Llama-3.2-1B-Instruct-Q4_0.gguf /models/ &&
      \$HOME/.wasmedge/bin/wasmedge --dir .:. --nn-preload default:GGML:AUTO:/models/Llama-3.2-1B-Instruct-Q4_0.gguf llama-api-server.wasm --model-name Llama-3.2-1B --prompt-template llama-3-chat --ctx-size 4096 --port 9068
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9068/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  whisper-api-server:
    image: ubuntu:22.04
    container_name: whisper-api-server
    ports:
      - "9069:9069"
    environment:
      - DEBIAN_FRONTEND=noninteractive
    volumes:
      - ./models:/models:ro
      - ./logs:/logs
    command: >
      bash -c "
      apt-get update && apt-get install -y curl wget tar &&
      curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -v 0.14.1 --ggmlbn=b5640 &&
      export PATH=\$PATH:\$HOME/.wasmedge/bin &&
      mkdir -p \$HOME/whisper/plugin &&
      cd \$HOME/whisper/plugin &&
      curl -LO https://github.com/WasmEdge/WasmEdge/releases/download/0.14.1/WasmEdge-plugin-wasi_nn-whisper-0.14.1-ubuntu20.04_x86_64.tar.gz &&
      tar -xzvf WasmEdge-plugin-wasi_nn-whisper-0.14.1-ubuntu20.04_x86_64.tar.gz &&
      cd /tmp &&
      curl -LO https://github.com/LlamaEdge/whisper-api-server/releases/download/0.4.1/whisper-api-server.wasm &&
      curl -LO https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-large-v2-q5_0.bin &&
      mkdir -p /models &&
      mv ggml-large-v2-q5_0.bin /models/ &&
      export WASMEDGE_PLUGIN_PATH=\$HOME/whisper/plugin &&
      \$HOME/.wasmedge/bin/wasmedge --dir .:. whisper-api-server.wasm -m /models/ggml-large-v2-q5_0.bin --port 9069
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9069/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      llama-api-server:
        condition: service_healthy

networks:
  default:
    name: llamaedge-network